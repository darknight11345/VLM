SLURM GPU devices: 0

CondaError: Run 'conda init' before 'conda deactivate'

INFO 08-08 14:04:22 [__init__.py:235] Automatically detected platform cuda.
INFO 08-08 14:04:57 [config.py:1604] Using max model len 1024000
INFO 08-08 14:05:02 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=16384.

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/Pixtral-Finetune/inference_scripts/test_tokenizer.py", line 3, in <module>
    llm = LLM(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 273, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 497, in from_engine_args
    return engine_cls.from_vllm_config(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 126, in from_vllm_config
    return cls(vllm_config=vllm_config,
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 88, in __init__
    self.tokenizer = init_tokenizer_from_configs(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer_group.py", line 111, in init_tokenizer_from_configs
    return TokenizerGroup(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer_group.py", line 24, in __init__
    self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer.py", line 227, in get_tokenizer
    tokenizer = MistralTokenizer.from_pretrained(str(tokenizer_name),
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/transformers_utils/tokenizers/mistral.py", line 254, in from_pretrained
    from mistral_common.tokens.tokenizers.mistral import (
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/mistral.py", line 16, in <module>
    from mistral_common.protocol.instruct.normalize import InstructRequestNormalizer, normalizer_for_tokenizer_version
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/mistral_common/protocol/instruct/normalize.py", line 22, in <module>
    from mistral_common.tokens.tokenizers.base import InstructRequestType, TokenizerVersion
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/base.py", line 22, in <module>
    from mistral_common.tokens.tokenizers.image import ImageEncoder
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/image.py", line 19, in <module>
    import cv2
AttributeError: _ARRAY_API not found
/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/tekken.py:337: FutureWarning: The attributed `special_token_policy` is deprecated and will be removed in 1.10.0. Please pass a special token policy explicitly to the relevant methods.
  warnings.warn(
WARNING 08-08 14:05:04 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 08-08 14:05:11 [__init__.py:235] Automatically detected platform cuda.
INFO 08-08 14:05:21 [config.py:1604] Using max model len 1024000
INFO 08-08 14:05:21 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=16384.

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<string>", line 1, in <module>
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/multiprocessing/spawn.py", line 125, in _main
    prepare(preparation_data)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/multiprocessing/spawn.py", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/multiprocessing/spawn.py", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/runpy.py", line 289, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/runpy.py", line 96, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/Pixtral-Finetune/inference_scripts/test_tokenizer.py", line 3, in <module>
    llm = LLM(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 273, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 152, in from_engine_args
    return cls(vllm_config=vllm_config,
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 88, in __init__
    self.tokenizer = init_tokenizer_from_configs(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer_group.py", line 111, in init_tokenizer_from_configs
    return TokenizerGroup(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer_group.py", line 24, in __init__
    self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer.py", line 227, in get_tokenizer
    tokenizer = MistralTokenizer.from_pretrained(str(tokenizer_name),
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/transformers_utils/tokenizers/mistral.py", line 254, in from_pretrained
    from mistral_common.tokens.tokenizers.mistral import (
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/mistral.py", line 16, in <module>
    from mistral_common.protocol.instruct.normalize import InstructRequestNormalizer, normalizer_for_tokenizer_version
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/mistral_common/protocol/instruct/normalize.py", line 22, in <module>
    from mistral_common.tokens.tokenizers.base import InstructRequestType, TokenizerVersion
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/base.py", line 22, in <module>
    from mistral_common.tokens.tokenizers.image import ImageEncoder
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/image.py", line 19, in <module>
    import cv2
AttributeError: _ARRAY_API not found
/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/tekken.py:337: FutureWarning: The attributed `special_token_policy` is deprecated and will be removed in 1.10.0. Please pass a special token policy explicitly to the relevant methods.
  warnings.warn(
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/multiprocessing/spawn.py", line 125, in _main
    prepare(preparation_data)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/multiprocessing/spawn.py", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/multiprocessing/spawn.py", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/runpy.py", line 289, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/runpy.py", line 96, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/Pixtral-Finetune/inference_scripts/test_tokenizer.py", line 3, in <module>
    llm = LLM(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 273, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 152, in from_engine_args
    return cls(vllm_config=vllm_config,
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 103, in __init__
    self.engine_core = EngineCoreClient.make_client(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 77, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 514, in __init__
    super().__init__(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 408, in __init__
    with launch_core_engines(vllm_config, executor_class,
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/contextlib.py", line 135, in __enter__
    return next(self.gen)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 680, in launch_core_engines
    local_engine_manager = CoreEngineProcManager(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 133, in __init__
    proc.start()
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/multiprocessing/context.py", line 288, in _Popen
    return Popen(process_obj)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/multiprocessing/popen_spawn_posix.py", line 42, in _launch
    prep_data = spawn.get_preparation_data(process_obj._name)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/multiprocessing/spawn.py", line 154, in get_preparation_data
    _check_not_importing_main()
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/multiprocessing/spawn.py", line 134, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
Traceback (most recent call last):
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/Pixtral-Finetune/inference_scripts/test_tokenizer.py", line 3, in <module>
    llm = LLM(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 273, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 497, in from_engine_args
    return engine_cls.from_vllm_config(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 126, in from_vllm_config
    return cls(vllm_config=vllm_config,
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 103, in __init__
    self.engine_core = EngineCoreClient.make_client(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 77, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 514, in __init__
    super().__init__(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 408, in __init__
    with launch_core_engines(vllm_config, executor_class,
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 697, in launch_core_engines
    wait_for_engine_startup(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 750, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

============================= JOB FEEDBACK =============================

NodeName=uc3n082
Job ID: 1095080
Cluster: uc3
User/Group: ul_swv79/ul_student
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 8
CPU Utilized: 00:00:36
CPU Efficiency: 2.66% of 00:22:32 core-walltime
Job Wall-clock time: 00:02:49
Memory Utilized: 1.25 GB
Memory Efficiency: 1.96% of 64.00 GB (64.00 GB/node)
