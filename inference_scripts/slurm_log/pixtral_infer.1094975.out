SLURM GPU devices: 0

CondaError: Run 'conda init' before 'conda deactivate'

INFO 08-08 13:57:20 [__init__.py:243] Automatically detected platform cuda.
INFO 08-08 13:57:24 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 08-08 13:57:24 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 08-08 13:57:24 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 08-08 13:57:37 [config.py:793] This model supports multiple tasks: {'reward', 'generate', 'classify', 'score', 'embed'}. Defaulting to 'generate'.
INFO 08-08 13:57:37 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=16384.
/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/mistral_common/tokens/tokenizers/tekken.py:337: FutureWarning: The attributed `special_token_policy` is deprecated and will be removed in 1.10.0. Please pass a special token policy explicitly to the relevant methods.
  warnings.warn(
INFO 08-08 13:57:39 [core.py:438] Waiting for init message from front-end.
INFO 08-08 13:57:39 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='/pfs/work9/workspace/scratch/ul_swv79-pixtral/pixtral-12b/', speculative_config=None, tokenizer='/pfs/work9/workspace/scratch/ul_swv79-pixtral/pixtral-12b/', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/pfs/work9/workspace/scratch/ul_swv79-pixtral/pixtral-12b/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["none"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}
WARNING 08-08 13:57:41 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x146d437aab60>
INFO 08-08 13:57:44 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
ERROR 08-08 13:57:47 [core.py:500] EngineCore failed to start.
ERROR 08-08 13:57:47 [core.py:500] Traceback (most recent call last):
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 491, in run_engine_core
ERROR 08-08 13:57:47 [core.py:500]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 390, in __init__
ERROR 08-08 13:57:47 [core.py:500]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 71, in __init__
ERROR 08-08 13:57:47 [core.py:500]     self.model_executor = executor_class(vllm_config)
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 08-08 13:57:47 [core.py:500]     self._init_executor()
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 46, in _init_executor
ERROR 08-08 13:57:47 [core.py:500]     self.collective_rpc("init_device")
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 08-08 13:57:47 [core.py:500]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/utils.py", line 2605, in run_method
ERROR 08-08 13:57:47 [core.py:500]     return func(*args, **kwargs)
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 604, in init_device
ERROR 08-08 13:57:47 [core.py:500]     self.worker.init_device()  # type: ignore
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 144, in init_device
ERROR 08-08 13:57:47 [core.py:500]     self.model_runner: GPUModelRunner = GPUModelRunner(
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 126, in __init__
ERROR 08-08 13:57:47 [core.py:500]     encoder_compute_budget, encoder_cache_size = compute_encoder_budget(
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/core/encoder_cache_manager.py", line 94, in compute_encoder_budget
ERROR 08-08 13:57:47 [core.py:500]     ) = _compute_encoder_budget_multimodal(
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/core/encoder_cache_manager.py", line 124, in _compute_encoder_budget_multimodal
ERROR 08-08 13:57:47 [core.py:500]     .get_max_tokens_per_item_by_nonzero_modality(model_config)
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/multimodal/registry.py", line 157, in get_max_tokens_per_item_by_nonzero_modality
ERROR 08-08 13:57:47 [core.py:500]     self.get_max_tokens_per_item_by_modality(model_config).items()
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/multimodal/registry.py", line 131, in get_max_tokens_per_item_by_modality
ERROR 08-08 13:57:47 [core.py:500]     return profiler.get_mm_max_tokens(
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/multimodal/profiling.py", line 271, in get_mm_max_tokens
ERROR 08-08 13:57:47 [core.py:500]     mm_inputs = self._get_dummy_mm_inputs(seq_len, mm_counts)
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/multimodal/profiling.py", line 181, in _get_dummy_mm_inputs
ERROR 08-08 13:57:47 [core.py:500]     processor_inputs = factory.get_dummy_processor_inputs(
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/multimodal/profiling.py", line 106, in get_dummy_processor_inputs
ERROR 08-08 13:57:47 [core.py:500]     dummy_text = self.get_dummy_text(mm_counts)
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 190, in get_dummy_text
ERROR 08-08 13:57:47 [core.py:500]     processor = self.info.get_hf_processor()
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 286, in get_hf_processor
ERROR 08-08 13:57:47 [core.py:500]     return self.ctx.get_hf_processor(PixtralProcessor, **kwargs)
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/inputs/registry.py", line 125, in get_hf_processor
ERROR 08-08 13:57:47 [core.py:500]     return super().get_hf_processor(
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/inputs/registry.py", line 88, in get_hf_processor
ERROR 08-08 13:57:47 [core.py:500]     return cached_processor_from_config(
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/transformers_utils/processor.py", line 109, in cached_processor_from_config
ERROR 08-08 13:57:47 [core.py:500]     return cached_get_processor(
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/transformers_utils/processor.py", line 71, in get_processor
ERROR 08-08 13:57:47 [core.py:500]     processor = processor_factory.from_pretrained(
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/transformers/processing_utils.py", line 1308, in from_pretrained
ERROR 08-08 13:57:47 [core.py:500]     return cls.from_args_and_dict(args, processor_dict, **kwargs)
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/transformers/processing_utils.py", line 1109, in from_args_and_dict
ERROR 08-08 13:57:47 [core.py:500]     processor = cls(*args, **valid_kwargs)
ERROR 08-08 13:57:47 [core.py:500]   File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/transformers/models/pixtral/processing_pixtral.py", line 111, in __init__
ERROR 08-08 13:57:47 [core.py:500]     self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)
ERROR 08-08 13:57:47 [core.py:500] AttributeError: 'MistralTokenizer' object has no attribute 'convert_tokens_to_ids'. Did you mean: 'convert_tokens_to_string'?
Process EngineCore_0:
Traceback (most recent call last):
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 504, in run_engine_core
    raise e
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 491, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 390, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 71, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 46, in _init_executor
    self.collective_rpc("init_device")
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/utils.py", line 2605, in run_method
    return func(*args, **kwargs)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 604, in init_device
    self.worker.init_device()  # type: ignore
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 144, in init_device
    self.model_runner: GPUModelRunner = GPUModelRunner(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py", line 126, in __init__
    encoder_compute_budget, encoder_cache_size = compute_encoder_budget(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/core/encoder_cache_manager.py", line 94, in compute_encoder_budget
    ) = _compute_encoder_budget_multimodal(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/core/encoder_cache_manager.py", line 124, in _compute_encoder_budget_multimodal
    .get_max_tokens_per_item_by_nonzero_modality(model_config)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/multimodal/registry.py", line 157, in get_max_tokens_per_item_by_nonzero_modality
    self.get_max_tokens_per_item_by_modality(model_config).items()
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/multimodal/registry.py", line 131, in get_max_tokens_per_item_by_modality
    return profiler.get_mm_max_tokens(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/multimodal/profiling.py", line 271, in get_mm_max_tokens
    mm_inputs = self._get_dummy_mm_inputs(seq_len, mm_counts)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/multimodal/profiling.py", line 181, in _get_dummy_mm_inputs
    processor_inputs = factory.get_dummy_processor_inputs(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/multimodal/profiling.py", line 106, in get_dummy_processor_inputs
    dummy_text = self.get_dummy_text(mm_counts)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 190, in get_dummy_text
    processor = self.info.get_hf_processor()
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 286, in get_hf_processor
    return self.ctx.get_hf_processor(PixtralProcessor, **kwargs)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/inputs/registry.py", line 125, in get_hf_processor
    return super().get_hf_processor(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/inputs/registry.py", line 88, in get_hf_processor
    return cached_processor_from_config(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/transformers_utils/processor.py", line 109, in cached_processor_from_config
    return cached_get_processor(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/transformers_utils/processor.py", line 71, in get_processor
    processor = processor_factory.from_pretrained(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/transformers/processing_utils.py", line 1308, in from_pretrained
    return cls.from_args_and_dict(args, processor_dict, **kwargs)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/transformers/processing_utils.py", line 1109, in from_args_and_dict
    processor = cls(*args, **valid_kwargs)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/transformers/models/pixtral/processing_pixtral.py", line 111, in __init__
    self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)
AttributeError: 'MistralTokenizer' object has no attribute 'convert_tokens_to_ids'. Did you mean: 'convert_tokens_to_string'?
Traceback (most recent call last):
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/Pixtral-Finetune/inference_scripts/test_tokenizer.py", line 3, in <module>
    llm = LLM(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/utils.py", line 1183, in inner
    return fn(*args, **kwargs)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 253, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 501, in from_engine_args
    return engine_cls.from_vllm_config(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 123, in from_vllm_config
    return cls(vllm_config=vllm_config,
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 100, in __init__
    self.engine_core = EngineCoreClient.make_client(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 75, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 580, in __init__
    super().__init__(
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 418, in __init__
    self._wait_for_engine_startup(output_address, parallel_config)
  File "/pfs/work9/workspace/scratch/ul_swv79-pixtral/conda/pixtral_inference/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 484, in _wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

============================= JOB FEEDBACK =============================

NodeName=uc3n082
Job ID: 1094975
Cluster: uc3
User/Group: ul_swv79/ul_student
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 8
CPU Utilized: 00:00:25
CPU Efficiency: 5.30% of 00:07:52 core-walltime
Job Wall-clock time: 00:00:59
Memory Utilized: 785.09 MB
Memory Efficiency: 1.20% of 64.00 GB (64.00 GB/node)
